{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import numpy as np\n",
    "\n",
    "# Load your dataset\n",
    "data_path = ''\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# Cleaning and preprocessing\n",
    "data.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "data.dropna(inplace=True)\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(data['label'])\n",
    "X = data.drop('label', axis=1)\n",
    "\n",
    "# Splitting the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Preprocessing pipeline\n",
    "pipeline = ImbPipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('smote', SMOTE(random_state=42)),\n",
    "    ('scaler', StandardScaler()),\n",
    "])\n",
    "pipeline.fit(X_train, y_train)\n",
    "X_train_transformed = pipeline.transform(X_train)\n",
    "X_test_transformed = pipeline.transform(X_test)\n",
    "\n",
    "# XGBoost classifier\n",
    "classifier = xgb.XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')\n",
    "classifier.fit(X_train_transformed, y_train)\n",
    "\n",
    "# Prediction and evaluation\n",
    "y_pred = classifier.predict(X_test_transformed)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy on test set:\", accuracy)\n",
    "print(\"Classification Report on test set:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Feature importances\n",
    "importances = classifier.feature_importances_\n",
    "features = X.columns\n",
    "importances_df = pd.DataFrame({'Feature': features, 'Importance': importances})\n",
    "sorted_importances = importances_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Display sorted feature importances\n",
    "print(\"Sorted Feature Importances:\")\n",
    "print(sorted_importances)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "import xgboost as xgb\n",
    "\n",
    "# Initialize results list to store accuracies for different feature counts\n",
    "results = []\n",
    "\n",
    "# The transformed training set may not retain the feature names, we need to reassign them\n",
    "X_train_transformed = pd.DataFrame(X_train_transformed, columns=X.columns)\n",
    "X_test_transformed = pd.DataFrame(X_test_transformed, columns=X.columns)\n",
    "\n",
    "# Loop through feature subsets from top 5 to top 20 features\n",
    "for top_n in range(30, 31):  # Adjust range to top 20\n",
    "    # Select the top 'top_n' features based on importance\n",
    "    top_features = sorted_importances['Feature'].head(top_n).tolist()  # Adjust variable name to sorted_importances\n",
    "    \n",
    "    # Subset the training and testing sets to the top 'top_n' features\n",
    "    X_train_reduced = X_train_transformed[top_features]\n",
    "    X_test_reduced = X_test_transformed[top_features]\n",
    "\n",
    "    # Scale the reduced feature sets\n",
    "    scaler = StandardScaler()\n",
    "    X_train_reduced_scaled = scaler.fit_transform(X_train_reduced)\n",
    "    X_test_reduced_scaled = scaler.transform(X_test_reduced)\n",
    "\n",
    "    # Reinitialize and retrain the XGBoost Classifier on reduced feature set\n",
    "    model_reduced = xgb.XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42)\n",
    "    model_reduced.fit(X_train_reduced_scaled, y_train)\n",
    "\n",
    "    # Make predictions with the reduced model\n",
    "    y_pred_reduced = model_reduced.predict(X_test_reduced_scaled)\n",
    "\n",
    "    # Calculate and store accuracy\n",
    "    accuracy_reduced = accuracy_score(y_test, y_pred_reduced)\n",
    "    results.append((top_n, accuracy_reduced))\n",
    "    \n",
    "    # Print the accuracy after each model run\n",
    "    print(f\"Top {top_n} Features Model Accuracy: {accuracy_reduced:.4f}\")\n",
    "\n",
    "# Print the final list of accuracies for each feature count\n",
    "print(\"\\nFinal List of Accuracies for Each Feature Count:\")\n",
    "for result in results:\n",
    "    print(f\"Top {result[0]} Features: Accuracy = {result[1]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ame",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
